# Approximate-Dynamic-Programming-and-Reinforcement-Learning

PLEASE SEE THE README FILES IN THE SUBFOLDERS REGARDING CODE DESCRIPTION.

This course was all about understanding Dynamic Programming algorithms and their problems regarding complexity. 
I further studied methods how these problems can be handled by approximating the Value or Policy vectors in a smaller subspace (Approximation).
At the end of the course we dived into modern Reinforcement Learning algorithms and techniques for approximating them. We learned about the following algorithms and their pros and cons:
- TD-Learning
- Q-Learning
- SARSA

The course was besides the ML lecture one of the most challenging for me, but I could learn a lot from it. I received a final grade of 1.3 (A).

### Content

The following text is taken out of the course description [1].

>Approximate dynamic programming (ADP) and reinforcement learning (RL) are two closely related paradigms for solving sequential decision making problems. ADP methods tackle the problems by developing optimal control methods that adapt to uncertain systems over time, while RL algorithms take the perspective of an agent that optimizes its behavior by interacting with its environment and learning from the feedback received. Both technologies have succeeded in applications of operation research, robotics, game playing, network management, and computational intelligence.

>We will cover the following topics (not exclusively):
>- Markov decision processes
>- Dynamic programming
>- Approximate dynamic programming
>- Reinforcement learning
>- Policy gradient algorithms
>- (Partially observable Markov decision processes) 

### Expected Learning Outcomes

The following text is taken out of the course description [1].

>On completion of this course, students are able to:
>- describe classic scenarios in sequential decision making problems;
>- explain basic models of ADP/RL methods;
>- derive ADP/RL algorithms that are covered in the course;
>- characterize convergence properties of the ADP/RL algorithms covered in the course;
>- compare performance of the ADP/RL algorithms that are covered in the course, both theoretically and practically;
>- select proper ADP/RL algorithms in accordance with specific applications;
>- construct and implement ADP/RL algorithms to solve simple simulated decision making problems.

#### References

[1] [Course description by the chair for data processing at Technical University Munich (TUM)](https://campus.tum.de/tumonline/wbLv.wbShowLVDetail?pStpSpNr=950432235)
